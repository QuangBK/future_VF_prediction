{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from cnn_finetune import make_model\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose,\n",
    "    RandomCrop, Normalize, Resize\n",
    ")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    return torch.mean(torch.mean((input - target) ** 2, dim = -1)*weight)\n",
    "\n",
    "class custom_MSE_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target, angles):\n",
    "        loss = torch.mean(((1+torch.abs(angles)**1.5)*torch.mean((output - target)**2, axis=1)))\n",
    "        return loss\n",
    "\n",
    "class Histogram_loss(torch.nn.Module):\n",
    "    def __init__(self, K_HL = 5):\n",
    "        super().__init__()\n",
    "        self.K_HL = K_HL\n",
    "        self.sf = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = -torch.mean(target.reshape(-1,NUM_CLASSES, self.K_HL)*self.sf(output.reshape(-1,NUM_CLASSES, self.K_HL)).log())\n",
    "        return loss\n",
    "\n",
    "class L2_SM_loss(torch.nn.Module):\n",
    "    def __init__(self, K_HL = 5):\n",
    "        super().__init__()\n",
    "        self.K_HL = K_HL\n",
    "        self.sf = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        loss = torch.mean((self.sf(output.reshape(-1,NUM_CLASSES, self.K_HL)) - target.reshape(-1,NUM_CLASSES, self.K_HL))**2)\n",
    "        return loss\n",
    "\n",
    "def show(img, size = 8):\n",
    "    plt.figure(figsize=(size,size))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "def show_VF(data, font_size = 4, size=10):\n",
    "    count = 0\n",
    "    VF_map = np.zeros((10,10))\n",
    "    fig = plt.figure(figsize=(size,size))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for i in range(0,10):\n",
    "        if i < 5:\n",
    "            k = 3-i\n",
    "        else:\n",
    "            k = i-6\n",
    "        if k < 0:\n",
    "            k = 0\n",
    "        for j in range(k,10-k):\n",
    "            VF_map[i,j] = data[count]\n",
    "            ax.annotate(\"{:.2f}\".format(data[count]),xy=(j-0.3,i), fontsize=font_size)   \n",
    "            count += 1                 \n",
    "    plt.imshow(VF_map)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def show_VF_paired(data_GT, data_pred, font_size = 4, size=15):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(size,size))\n",
    "    show_VF_single(data_GT, ax[0], font_size = font_size)\n",
    "    ax[0].set_title(\"Ground truth\")\n",
    "    show_VF_single(data_pred, ax[1], font_size = font_size)\n",
    "    ax[1].set_title(\"Prediction\")\n",
    "\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def show_VF_single(data, ax, font_size = 4, size=10, decimal = 1):\n",
    "    count = 0\n",
    "    VF_map = np.zeros((10,10))\n",
    "    VF_map -= 1\n",
    "    for i in range(0,10):\n",
    "        if i < 5:\n",
    "            k = 3-i\n",
    "        else:\n",
    "            k = i-6\n",
    "        if k < 0:\n",
    "            k = 0\n",
    "        for j in range(k,10-k):\n",
    "            VF_map[i,j] = data[count]\n",
    "            if data[count] < 20:\n",
    "                ax.annotate((\"{:.\" + str(decimal) + \"f}\").format(data[count]),xy=(j,i), fontsize=font_size, color='whitesmoke', ha='center')\n",
    "            else:\n",
    "                ax.annotate((\"{:.\" + str(decimal) + \"f}\").format(data[count]),xy=(j,i), fontsize=font_size, color='black', ha='center') \n",
    "            count += 1\n",
    "    ax.imshow(VF_map, vmin=-1, vmax=40, cmap='viridis')\n",
    "    return None\n",
    "\n",
    "def show_VF_list(data, list_title = None, font_size = 4, size=15):    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(data), figsize=(size,5))\n",
    "    for i in range(len(data)):\n",
    "        show_VF_single(data[i], ax[i], font_size = font_size, decimal = 0)\n",
    "        if list_title is not None:\n",
    "            ax[i].set_title(list_title[i])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE_HD_loss(torch.nn.Module):\n",
    "    def __init__(self, TH = 0.7):\n",
    "        super().__init__()\n",
    "        self.TH = TH\n",
    "\n",
    "    def convert_vf1d_array2d(self,vf):\n",
    "        count = 0\n",
    "        # Use the all zeros array as initial array obtains best performance\n",
    "        VF_map = np.zeros((vf.shape[0], 10,10))\n",
    "        for i in range(0,10):\n",
    "            if i < 5:\n",
    "                k = 3-i\n",
    "            else:\n",
    "                k = i-6\n",
    "            if k < 0:\n",
    "                k = 0\n",
    "            for j in range(k,10-k):\n",
    "                VF_map[:,i,j] = vf[:,count]\n",
    "                count += 1\n",
    "        return VF_map\n",
    "    \n",
    "    def convert_vf2d_array1d(self,VF_map):\n",
    "        count = 0\n",
    "        vf = np.zeros((VF_map.shape[0],76))\n",
    "        for i in range(0,10):\n",
    "            if i < 5:\n",
    "                k = 3-i\n",
    "            else:\n",
    "                k = i-6\n",
    "            if k < 0:\n",
    "                k = 0\n",
    "            for j in range(k,10-k):\n",
    "                vf[:,count] = VF_map[:,i,j]\n",
    "                count += 1\n",
    "        return vf\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output_np = self.convert_vf1d_array2d(output.cpu().detach().numpy())*STD + MEAN\n",
    "        target_np = self.convert_vf1d_array2d(target.cpu().detach().numpy())*STD + MEAN\n",
    "        weight = np.zeros((target.shape[0],10,10))\n",
    "        for i in range(target.shape[0]):\n",
    "            _, bw = cv2.threshold(target_np[i].astype(np.uint8), 30, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "            dist = cv2.distanceTransform(bw, cv2.DIST_L2, 3)\n",
    "            _, dist = cv2.threshold(dist, 1, 1,cv2.THRESH_BINARY)\n",
    "            diff = np.where((target_np[i]-output_np[i]) > 0, 1, 0)\n",
    "            weight[i] = np.where((1-dist)*diff, self.TH, 1)\n",
    "        weight = self.convert_vf2d_array1d(weight)\n",
    "        weight = torch.from_numpy(weight).cuda()\n",
    "        loss = torch.mean(torch.mean((output - target) ** 2*weight, dim = -1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_STATE = 94\n",
    "NUM_DIM = 10\n",
    "IMG_SIZE = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_CHANNELS = 3\n",
    "NUM_CLASSES = 76\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 25\n",
    "MEAN = 24.6\n",
    "STD = 5\n",
    "K_HL = 10\n",
    "MAX_VF = 3.88\n",
    "MIN_VF = -5.12\n",
    "\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101\n",
    "SHIFT_LIMIT = 0.01\n",
    "SCALE_LIMIT = 0.01\n",
    "ROTATE_LIMIT = 5\n",
    "SHIFT_LIMIT_TOMO = 0.01\n",
    "SCALE_LIMIT_TOMO = 0.01\n",
    "ROTATE_LIMIT_TOMO = 5\n",
    "AUG_P = 0.5\n",
    "HOR_P = 1.\n",
    "\n",
    "SCALE_H = 1.1\n",
    "SCALE_W = 1.3\n",
    "\n",
    "PATH = '/home/quang/working/OCT_images/oct_dataset/'\n",
    "TH_REG_OUTPUT = 1.5\n",
    "MODEL_NAME = 'resnet50_1input_regions_MSELoss_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_COL = []\n",
    "for i in range(1,77):\n",
    "    LIST_COL.append('GS_' + str(i))\n",
    "    \n",
    "count = 0\n",
    "LIST_COL_INVERS = []\n",
    "row_temp = 0\n",
    "for i in range(0,10):\n",
    "    if i < 5:\n",
    "        k = 3-i\n",
    "    else:\n",
    "        k = i-6\n",
    "    if k < 0:\n",
    "        k = 0\n",
    "    row_temp = count\n",
    "    for j in range(2*(5-k),0,-1):\n",
    "        LIST_COL_INVERS.append('GS_' + str(j+row_temp))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (MAX_VF-MIN_VF)/K_HL\n",
    "STD_VF = math.sqrt(step_size/2)\n",
    "P_DIS = {}\n",
    "for i in range(-1,45):\n",
    "    temp_norm_value = (i - MEAN)/STD\n",
    "    list_pi = []\n",
    "    Z_temp = 0.5*(math.erf((MAX_VF-temp_norm_value)/(STD_VF*math.sqrt(2))) - math.erf((MIN_VF-temp_norm_value)/(STD_VF*math.sqrt(2))))\n",
    "    for j in range(K_HL):\n",
    "        l_i = MIN_VF + j*step_size\n",
    "        p_j = 0.5*(math.erf((l_i+step_size-temp_norm_value)/(STD_VF*math.sqrt(2))) - math.erf((l_i-temp_norm_value)/(STD_VF*math.sqrt(2))))/Z_temp\n",
    "        list_pi.append(p_j)\n",
    "    P_DIS[temp_norm_value] = list_pi\n",
    "    \n",
    "LIST_VALUES = []\n",
    "for j in range(K_HL):\n",
    "    l_i = MIN_VF + j*step_size\n",
    "    LIST_VALUES.append(l_i + 0.5*step_size)\n",
    "LIST_VALUES = torch.tensor(LIST_VALUES).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_external = pd.read_csv('/home/quang/working/OCT_images/GOOD_NOTEBOOK/external_data_format/external_data_formated_for_evaluation.csv')\n",
    "df_external = df_external.astype({'test_date': object})\n",
    "df_external.test_date = df_external.test_date.astype(str)\n",
    "df_external['string_path'] = None\n",
    "df_external['noise'] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all, df_train, df_test, df_angle = get_dataset_df_all_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['patient_id'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)\n",
    "\n",
    "df_all_removed = df_all[(df_all['path_rnfl'] != 'None') & (df_all['path_gcipl'] != 'None')]\n",
    "len(df_all_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('df_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = df_all.groupby(['patient_id', 'OD_OS'])\n",
    "list_id_lr = []\n",
    "for name, group in g1:\n",
    "    list_id_lr.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = group.sort_values(by=['timeline'])\n",
    "\n",
    "time_first = temp_df.iloc[0]['timeline']\n",
    "label_list = []\n",
    "for index, row in temp_df.iterrows():\n",
    "    label_list.append(np.array(list(row[LIST_COL])))\n",
    "    \n",
    "label_list = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['noise'] = 1.\n",
    "df_all['noise_ae'] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = not df_all[\"string_path\"].is_unique  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FundusDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, df_angle, transform=None, transform_flip=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "#         self.df = df\n",
    "        self.df = df[(df['path_rnfl'] != 'None') & (df['path_gcipl'] != 'None') & (df['string_path'] != 'None')]\n",
    "        self.df_angle = df_angle\n",
    "        self.transform = transform\n",
    "        self.transform_flip = transform_flip\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        row = self.df.iloc[idx]\n",
    "        angle_label_t = self.df_angle[self.df_angle['string_path']==row['string_path']]['angle_od_refined']\n",
    "        if len(angle_label_t) > 0:\n",
    "            angle_label = angle_label_t.iloc[0] - 7.264731\n",
    "        else:\n",
    "            angle_label = 0\n",
    "        image_rnfl = cv2.imread(row['path_rnfl'])\n",
    "        image_rnfl = cv2.cvtColor(image_rnfl, cv2.COLOR_BGR2RGB)\n",
    "        image = image_rnfl\n",
    "        if row['OD_OS'] == 'OD':\n",
    "            if self.transform:\n",
    "                label = np.array(list(row[LIST_COL]))\n",
    "                transformed = self.transform(image=image)\n",
    "\n",
    "        else:            \n",
    "            if self.transform:\n",
    "                label = np.array(list(row[LIST_COL_INVERS]))\n",
    "                transformed = self.transform_flip(image=image)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        label = (label - MEAN)/STD       \n",
    "        sample = {'image': image, 'label': torch.tensor(label).float(),'string_path': row['string_path'], 'file': row['path_rnfl'], 'angle': torch.tensor(angle_label).float()}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "def get_dataloader_noise(df_train, df_test, df_all, df_angle):    \n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    #     A.Normalize((0.485, 0.456, 0.406), 0.229, 0.224, 0.225),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    test_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    train_dataset = FundusDataset(df_train, df_angle, transform=train_transform, transform_flip=train_transform_flip)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                 batch_size=16, shuffle=True,\n",
    "                                                 num_workers=4, drop_last=True)\n",
    "\n",
    "    test_dataset = FundusDataset(df_test, df_angle, transform=test_transform, transform_flip=test_transform_flip)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                 batch_size=16, shuffle=False,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "\n",
    "    test_dataset_all = FundusDataset(df_all, df_angle, transform=test_transform, transform_flip=test_transform_flip)\n",
    "\n",
    "    test_loader_all = torch.utils.data.DataLoader(test_dataset_all,\n",
    "                                                 batch_size=16, shuffle=False,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "    dataloaders = {'train' : train_loader, 'val' : test_loader}\n",
    "\n",
    "    dataset_sizes = {'train' : len(train_dataset), 'val' : len(test_dataset)}\n",
    "    return dataloaders, dataset_sizes, test_loader_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCT_Series_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, df_angle, transform=None, transform_flip=None,\n",
    "                transform_tomo=None, transform_flip_tomo=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "#         self.df = df[(df['path_rnfl'] != 'None') & (df['path_gcipl'] != 'None')]\n",
    "        self.df_angle = df_angle\n",
    "        self.transform = transform\n",
    "        self.transform_flip = transform_flip\n",
    "        self.transform_tomo = transform_tomo\n",
    "        self.transform_flip_tomo = transform_flip_tomo\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        g1 = self.df.groupby(['patient_id', 'OD_OS'])\n",
    "        self.list_id_lr = []\n",
    "        for name, group in g1:\n",
    "            temp_df = self.df[(self.df['patient_id'] == name[0]) & (self.df['OD_OS'] == name[1])]\n",
    "            if len(temp_df) > 1:\n",
    "                self.list_id_lr.append(name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_id_lr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        temp_df = df_all[(df_all['patient_id'] == self.list_id_lr[idx][0]) & (df_all['OD_OS'] == self.list_id_lr[idx][1])].sort_values(by=['timeline'])\n",
    "        time_first = temp_df.iloc[0]['timeline']\n",
    "        if self.list_id_lr[idx][1] == 'OD':\n",
    "            list_col_temp = LIST_COL\n",
    "            transform_temp = self.transform\n",
    "        else:\n",
    "            list_col_temp = LIST_COL_INVERS\n",
    "            transform_temp = self.transform_flip\n",
    "\n",
    "        list_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        input_list = []\n",
    "        label_list = []\n",
    "        total_list = []\n",
    "        count_temp = 0\n",
    "        string_paths = []\n",
    "        list_noise = []\n",
    "        list_noise_ae = []\n",
    "#         print (len(temp_df))\n",
    "        for index, row in temp_df.iterrows():\n",
    "            data_temp = list(row[list_col_temp])\n",
    "            data_temp.insert(0, (row['timeline'] - time_first)/12.)\n",
    "            \n",
    "            image_rnfl = cv2.imread(row['path_rnfl'])\n",
    "            image_rnfl = cv2.cvtColor(image_rnfl, cv2.COLOR_BGR2RGB)\n",
    "            image = image_rnfl\n",
    "        \n",
    "            transformed = transform_temp(image=image)\n",
    "            image = transformed['image']\n",
    "            if count_temp > 0:\n",
    "                label_list.append(np.array(data_temp))\n",
    "            if count_temp < (len(temp_df) - 1):\n",
    "                input_list.append(np.array(data_temp))\n",
    "            list_imgs = torch.cat([list_imgs, image.unsqueeze(0)], dim = 0)\n",
    "            total_list.append(np.array(data_temp))\n",
    "            string_paths.append(row['string_path'])\n",
    "            list_noise.append(row['noise'])\n",
    "            list_noise_ae.append(row['noise_ae'])\n",
    "            count_temp += 1\n",
    "\n",
    "        label_list = np.array(label_list).astype(float)\n",
    "        label_list = (label_list - MEAN)/STD\n",
    "        \n",
    "        input_list = np.array(input_list).astype(float)\n",
    "        input_list = (input_list - MEAN)/STD\n",
    "        \n",
    "        total_list = np.array(total_list).astype(float)\n",
    "        total_list = (total_list - MEAN)/STD\n",
    "        \n",
    "        sample = {'input': torch.tensor(input_list).float(), 'label': torch.tensor(label_list[:,1:]).float(), 'imgs': list_imgs,\n",
    "                  'total_vfs': torch.tensor(total_list[:,1:]).float(), 'string_path': string_paths,\n",
    "                  'noise': torch.tensor(list_noise).float(), 'noise_ae': torch.tensor(list_noise_ae).float()}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "def get_dataloader(df_train, df_test, df_all, df_angle):   \n",
    "    train_transform_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    #     A.Normalize((0.485, 0.456, 0.406), 0.229, 0.224, 0.225),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_transform_flip_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    test_transform_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    test_transform_flip_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    #     A.Normalize((0.485, 0.456, 0.406), 0.229, 0.224, 0.225),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    #     A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    test_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "    #     A.VerticalFlip(p=.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=1.),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    train_dataset = OCT_Series_Dataset(df_train, df_angle, transform=train_transform, transform_flip=train_transform_flip,\n",
    "                                 transform_tomo=train_transform_tomo, transform_flip_tomo=train_transform_flip_tomo)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                 batch_size=1, shuffle=True,\n",
    "                                                 num_workers=1)\n",
    "\n",
    "    test_dataset = OCT_Series_Dataset(df_test, df_angle, transform=test_transform, transform_flip=test_transform_flip,\n",
    "                                transform_tomo=test_transform_tomo, transform_flip_tomo=test_transform_flip_tomo)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                 batch_size=1, shuffle=False,\n",
    "                                                 num_workers=1)\n",
    "\n",
    "\n",
    "    test_dataset_all = OCT_Series_Dataset(df_all, df_angle, transform=test_transform, transform_flip=test_transform_flip,\n",
    "                                    transform_tomo=test_transform_tomo, transform_flip_tomo=test_transform_flip_tomo)\n",
    "\n",
    "    test_loader_all = torch.utils.data.DataLoader(test_dataset_all,\n",
    "                                                 batch_size=1, shuffle=False,\n",
    "                                                 num_workers=1)\n",
    "\n",
    "    dataloaders = {'train' : train_loader, 'val' : test_loader}\n",
    "\n",
    "    dataset_sizes = {'train' : len(train_dataset), 'val' : len(test_dataset)}\n",
    "    return dataloaders, dataset_sizes, test_loader_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCT_Series_ver2_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, df_all, transform=None, transform_flip=None,\n",
    "                transform_tomo=None, transform_flip_tomo=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.df_all = df_all\n",
    "        self.transform = transform\n",
    "        self.transform_flip = transform_flip\n",
    "        self.transform_tomo = transform_tomo\n",
    "        self.transform_flip_tomo = transform_flip_tomo\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        g1 = self.df.groupby(['patient_id', 'OD_OS'])\n",
    "        self.list_id_lr = []\n",
    "        for name, group in g1:\n",
    "            temp_df = self.df[(self.df['patient_id'] == name[0]) & (self.df['OD_OS'] == name[1])]\n",
    "            if len(temp_df) > 1:\n",
    "                self.list_id_lr.append(name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_id_lr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        temp_df = self.df_all[(self.df_all['patient_id'] == self.list_id_lr[idx][0]) & (self.df_all['OD_OS'] == self.list_id_lr[idx][1])].sort_values(by=['timeline'])\n",
    "        time_first = temp_df.iloc[0]['timeline']\n",
    "        if self.list_id_lr[idx][1] == 'OD':\n",
    "            list_col_temp = LIST_COL\n",
    "            transform_temp = self.transform\n",
    "            transform_vertical = self.transform_tomo\n",
    "            transform_horizontal = self.transform_tomo\n",
    "        else:\n",
    "            list_col_temp = LIST_COL_INVERS\n",
    "            transform_temp = self.transform_flip\n",
    "            transform_vertical = self.transform_tomo\n",
    "            transform_horizontal = self.transform_flip_tomo\n",
    "\n",
    "        list_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        list__vertical_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        list_horizontal_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        input_list = []\n",
    "        label_list = []\n",
    "        total_list = []\n",
    "        count_temp = 0\n",
    "        string_paths = []\n",
    "        list_noise = []\n",
    "        list_noise_ae = []\n",
    "        list_timeline = []\n",
    "        list_intervals = []\n",
    "        list_file = []\n",
    "        list_vfi = []\n",
    "        current_time = time_first\n",
    "        missing_image = np.zeros((IMG_SIZE,IMG_SIZE,3), dtype=np.uint8)\n",
    "        for index, row in temp_df.iterrows():\n",
    "            data_temp = list(row[list_col_temp])\n",
    "            temp_interval = (row['timeline'] - time_first)/12.\n",
    "            data_temp.insert(0, temp_interval)\n",
    "            list_timeline.append((row['timeline'] - time_first)/12.)\n",
    "#             image_horizontal = cv2.imread(row['path_rnfl'][:-4] + '_horizontalTomo.jpg')\n",
    "#             image_vertical = cv2.imread(row['path_rnfl'][:-4] + '_verticalTomo.jpg')        \n",
    "#             image_rnfl = cv2.imread(row['path_rnfl'])\n",
    "            \n",
    "#             image_rnfl = cv2.cvtColor(image_rnfl, cv2.COLOR_BGR2RGB)\n",
    "#             image_vertical = cv2.cvtColor(image_vertical, cv2.COLOR_BGR2RGB)\n",
    "#             image_horizontal = cv2.cvtColor(image_horizontal, cv2.COLOR_BGR2RGB)\n",
    "#             image = image_rnfl\n",
    "            \n",
    "#             transformed = transform_temp(image=image)\n",
    "#             image = transformed['image']\n",
    "            \n",
    "#             transformed = transform_vertical(image=image_vertical)\n",
    "#             image_vertical = transformed['image']\n",
    "            \n",
    "#             transformed = transform_temp(image=image_horizontal)\n",
    "#             image_horizontal = transformed['image']\n",
    "            \n",
    "            if row['path_rnfl'] == 'None':\n",
    "                image_rnfl = missing_image.copy()\n",
    "            else:\n",
    "                image_rnfl = cv2.imread(row['path_rnfl'])\n",
    "            image_rnfl = cv2.cvtColor(image_rnfl, cv2.COLOR_BGR2RGB)\n",
    "            image = image_rnfl\n",
    "            \n",
    "#             list_test_date.append(row['test_date'])\n",
    "            list_file.append(row['path_rnfl'])\n",
    "            \n",
    "            try:\n",
    "                list_vfi.append(re.sub(\"[^0-9]\", \"\", row['vfi']))\n",
    "            except  Exception as e:\n",
    "                print(e)\n",
    "                print (row['vfi'])\n",
    "            transformed = transform_temp(image=image)\n",
    "            image = transformed['image']\n",
    "            \n",
    "            if row['path_gcipl'] == 'None':\n",
    "                image_vertical = missing_image.copy()\n",
    "                image_horizontal = missing_image.copy()\n",
    "            else:\n",
    "                image_horizontal = cv2.imread(row['path_rnfl'][:-4] + '_horizontalTomo.jpg')\n",
    "                image_vertical = cv2.imread(row['path_rnfl'][:-4] + '_verticalTomo.jpg')        \n",
    "                image_vertical = cv2.cvtColor(image_vertical, cv2.COLOR_BGR2RGB)\n",
    "                image_horizontal = cv2.cvtColor(image_horizontal, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            transformed = transform_vertical(image=image_vertical)\n",
    "            image_vertical = transformed['image']\n",
    "\n",
    "            transformed = transform_temp(image=image_horizontal)\n",
    "            image_horizontal = transformed['image']\n",
    "            \n",
    "            list_file.append(row['path_rnfl'])\n",
    "            if count_temp > 0:\n",
    "                label_list.append(np.array(data_temp))\n",
    "                list_intervals.append((row['timeline'] - current_time)/12.)\n",
    "                current_time = row['timeline']\n",
    "\n",
    "            if count_temp < (len(temp_df) - 1):\n",
    "                input_list.append(np.array(data_temp))\n",
    "                \n",
    "            list_imgs = torch.cat([list_imgs, image.unsqueeze(0)], dim = 0)\n",
    "            list__vertical_imgs = torch.cat([list__vertical_imgs, image_vertical.unsqueeze(0)], dim = 0)\n",
    "            list_horizontal_imgs = torch.cat([list_horizontal_imgs, image_horizontal.unsqueeze(0)], dim = 0)\n",
    "\n",
    "            total_list.append(np.array(data_temp))\n",
    "            if row['string_path'] is None:\n",
    "                string_paths.append('None')\n",
    "            else:\n",
    "                string_paths.append(row['string_path'])\n",
    "#             string_paths.append(row['string_path'])\n",
    "            list_noise.append(row['noise'])\n",
    "            count_temp += 1\n",
    "\n",
    "        label_list = np.array(label_list).astype(float)\n",
    "        label_list[:,1:] = (label_list[:,1:] - MEAN)/STD\n",
    "        \n",
    "        input_list = np.array(input_list).astype(float)\n",
    "        input_list[:,1:] = (input_list[:,1:] - MEAN)/STD\n",
    "        \n",
    "        total_list = np.array(total_list).astype(float)\n",
    "        total_list[:,1:] = (total_list[:,1:] - MEAN)/STD\n",
    "\n",
    "        input_list[:-1,0] = input_list[1:,0]\n",
    "        input_list[-1,0] = label_list[-1,0]\n",
    "        \n",
    "        list_vfi = np.array(list_vfi).astype(float)\n",
    "        list_vfi = (list_vfi - 80)/10\n",
    "    \n",
    "        sample = {'input': torch.tensor(input_list).float(), 'label': torch.tensor(label_list[:,1:]).float(), 'imgs': list_imgs,\n",
    "                  'total_vfs': torch.tensor(total_list[:,1:]).float(), 'string_path': string_paths,\n",
    "                  'noise': torch.tensor(list_noise).float(),  'interval': torch.tensor(list_intervals).float(),\n",
    "                  'timeline': torch.tensor(list_timeline).float(), 'list_file': list_file, 'vertical_imgs': list__vertical_imgs, \n",
    "                  'horizontal_imgs': list_horizontal_imgs, 'list_vfi':  torch.tensor(list_vfi[1:]).float()}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "class OCT_Series_ver2_external_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, df_all, transform=None, transform_flip=None,\n",
    "                transform_tomo=None, transform_flip_tomo=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.df_all = df_all\n",
    "        self.transform = transform\n",
    "        self.transform_flip = transform_flip\n",
    "        self.transform_tomo = transform_tomo\n",
    "        self.transform_flip_tomo = transform_flip_tomo\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        g1 = self.df.groupby(['patient_id', 'OD_OS'])\n",
    "        self.list_id_lr = []\n",
    "        for name, group in g1:\n",
    "            temp_df = self.df[(self.df['patient_id'] == name[0]) & (self.df['OD_OS'] == name[1])]\n",
    "            if len(temp_df) > 1:\n",
    "                # name = name + (len(temp_df),)\n",
    "                self.list_id_lr.append(name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_id_lr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        temp_df = self.df_all[(self.df_all['patient_id'] == self.list_id_lr[idx][0]) & (self.df_all['OD_OS'] == self.list_id_lr[idx][1])].sort_values(by=['timeline'])\n",
    "        time_first = temp_df.iloc[0]['timeline']\n",
    "        if self.list_id_lr[idx][1] == 'OD':\n",
    "            list_col_temp = LIST_COL\n",
    "            transform_temp = self.transform\n",
    "            transform_vertical = self.transform_tomo\n",
    "            transform_horizontal = self.transform_tomo\n",
    "        else:\n",
    "            list_col_temp = LIST_COL_INVERS\n",
    "            transform_temp = self.transform_flip\n",
    "            transform_vertical = self.transform_tomo\n",
    "            transform_horizontal = self.transform_flip_tomo\n",
    "        \n",
    "        list_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        list__vertical_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        list_horizontal_imgs = torch.zeros([0, 3, IMG_SIZE, IMG_SIZE], dtype=torch.float)\n",
    "        input_list = []\n",
    "        label_list = []\n",
    "        total_list = []\n",
    "        count_temp = 0\n",
    "        string_paths = []\n",
    "        list_noise = []\n",
    "        list_noise_ae = []\n",
    "        list_timeline = []\n",
    "        list_intervals = []\n",
    "        list_file = []\n",
    "        current_time = time_first\n",
    "        for index, row in temp_df.iterrows():\n",
    "            data_temp = list(row[list_col_temp])\n",
    "            temp_interval = (row['timeline'] - time_first)/12.\n",
    "            data_temp.insert(0, temp_interval)\n",
    "            list_timeline.append((row['timeline'] - time_first)/12.)\n",
    "            image_horizontal = cv2.imread(row['path_rnfl'][:-4] + '_horizontalTomo.jpg')\n",
    "            image_vertical = cv2.imread(row['path_rnfl'][:-4] + '_verticalTomo.jpg')        \n",
    "            image_rnfl = cv2.imread(row['path_rnfl'])\n",
    "            \n",
    "            image_rnfl = cv2.cvtColor(image_rnfl, cv2.COLOR_BGR2RGB)\n",
    "            image_vertical = cv2.cvtColor(image_vertical, cv2.COLOR_BGR2RGB)\n",
    "            image_horizontal = cv2.cvtColor(image_horizontal, cv2.COLOR_BGR2RGB)\n",
    "            image = image_rnfl\n",
    "            list_file.append(row['path_rnfl'])\n",
    "            transformed = transform_temp(image=image)\n",
    "            image = transformed['image']\n",
    "            \n",
    "            transformed = transform_vertical(image=image_vertical)\n",
    "            image_vertical = transformed['image']\n",
    "            \n",
    "            transformed = transform_temp(image=image_horizontal)\n",
    "            image_horizontal = transformed['image']\n",
    "            \n",
    "            if count_temp > 0:\n",
    "                label_list.append(np.array(data_temp))\n",
    "                list_intervals.append((row['timeline'] - current_time)/12.)\n",
    "                current_time = row['timeline']\n",
    "\n",
    "            if count_temp < (len(temp_df) - 1):\n",
    "                input_list.append(np.array(data_temp))\n",
    "                \n",
    "            list_imgs = torch.cat([list_imgs, image.unsqueeze(0)], dim = 0)\n",
    "            list__vertical_imgs = torch.cat([list__vertical_imgs, image_vertical.unsqueeze(0)], dim = 0)\n",
    "            list_horizontal_imgs = torch.cat([list_horizontal_imgs, image_horizontal.unsqueeze(0)], dim = 0)\n",
    "\n",
    "            total_list.append(np.array(data_temp))\n",
    "            # string_paths.append(row['string_path'])\n",
    "            string_paths.append('abc')\n",
    "            list_noise.append(row['noise'])\n",
    "            count_temp += 1\n",
    "\n",
    "        label_list = np.array(label_list).astype(float)\n",
    "        label_list[:,1:] = (label_list[:,1:] - MEAN)/STD\n",
    "        \n",
    "        input_list = np.array(input_list).astype(float)\n",
    "        input_list[:,1:] = (input_list[:,1:] - MEAN)/STD\n",
    "        \n",
    "        total_list = np.array(total_list).astype(float)\n",
    "        total_list[:,1:] = (total_list[:,1:] - MEAN)/STD\n",
    "\n",
    "        input_list[:-1,0] = input_list[1:,0]\n",
    "        input_list[-1,0] = label_list[-1,0]\n",
    "    \n",
    "        sample = {'input': torch.tensor(input_list).float(), 'label': torch.tensor(label_list[:,1:]).float(), 'imgs': list_imgs,\n",
    "                  'total_vfs': torch.tensor(total_list[:,1:]).float(), 'string_path': string_paths,\n",
    "                  'noise': torch.tensor(list_noise).float(),  'interval': torch.tensor(list_intervals).float(),\n",
    "                  'timeline': torch.tensor(list_timeline).float(), 'list_file': list_file, 'vertical_imgs': list__vertical_imgs, \n",
    "                  'horizontal_imgs': list_horizontal_imgs}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "def get_ver2_dataloader(df_train, df_test, df_all):   \n",
    "    train_transform_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT_TOMO, scale_limit=SCALE_LIMIT_TOMO, rotate_limit=ROTATE_LIMIT_TOMO, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_transform_flip_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT_TOMO, scale_limit=SCALE_LIMIT_TOMO, rotate_limit=ROTATE_LIMIT_TOMO, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    test_transform_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    test_transform_flip_tomo = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*SCALE_H), int(IMG_SIZE*SCALE_W)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "        A.ShiftScaleRotate(shift_limit=SHIFT_LIMIT, scale_limit=SCALE_LIMIT, rotate_limit=ROTATE_LIMIT, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    test_transform_flip = A.Compose([\n",
    "        A.Resize(int(IMG_SIZE*1.1), int(IMG_SIZE*1.1)),\n",
    "        A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=HOR_P),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=0, border_mode=BORDER_MODE, value=0, p=AUG_P),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    train_dataset = OCT_Series_ver2_Dataset(df_train, df_all, \n",
    "                                            transform=train_transform, transform_flip=train_transform_flip,\n",
    "                                            transform_tomo=train_transform_tomo, transform_flip_tomo=train_transform_flip_tomo)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                 batch_size=1, shuffle=True,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "    test_dataset = OCT_Series_ver2_Dataset(df_test, df_all,\n",
    "                                           transform=test_transform, transform_flip=test_transform_flip,\n",
    "                                           transform_tomo=test_transform_tomo, transform_flip_tomo=test_transform_flip_tomo)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                 batch_size=1, shuffle=False,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "\n",
    "    test_dataset_all = OCT_Series_ver2_Dataset(df_all, df_all, \n",
    "                                               transform=test_transform, transform_flip=test_transform_flip,\n",
    "                                               transform_tomo=test_transform_tomo, transform_flip_tomo=test_transform_flip_tomo)\n",
    "\n",
    "    test_loader_all = torch.utils.data.DataLoader(test_dataset_all,\n",
    "                                                 batch_size=1, shuffle=False,\n",
    "                                                 num_workers=4)\n",
    "    \n",
    "    test_external_dataset = OCT_Series_ver2_external_Dataset(df_external, df_external,\n",
    "                                           transform=test_transform, transform_flip=test_transform_flip,\n",
    "                                           transform_tomo=test_transform_tomo, transform_flip_tomo=test_transform_flip_tomo)\n",
    "\n",
    "    test_external_loader = torch.utils.data.DataLoader(test_external_dataset,\n",
    "                                                 batch_size=1, shuffle=False,\n",
    "                                                 num_workers=4)\n",
    "\n",
    "    dataloaders = {'train' : train_loader, 'val' : test_loader, 'external': test_external_loader}\n",
    "\n",
    "    dataset_sizes = {'train' : len(train_dataset), 'val' : len(test_dataset), 'external' : len(test_external_loader)}\n",
    "    return dataloaders, dataset_sizes, test_loader_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze(model, ct_c = 7):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    ct = 0\n",
    "    for child in model.features[0].children():\n",
    "        ct += 1\n",
    "        if ct < ct_c:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "class BoundedGridLocNet(nn.Module):\n",
    "\n",
    "    def __init__(self, grid_height, grid_width, target_control_points):\n",
    "        super(BoundedGridLocNet, self).__init__()\n",
    "        self.cnn = CNN(grid_height * grid_width * 2)\n",
    "\n",
    "        bias = torch.from_numpy(np.arctanh(target_control_points.numpy()))\n",
    "        bias = bias.view(-1)\n",
    "        self.cnn.fc2.bias.data.copy_(bias)\n",
    "        self.cnn.fc2.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        points = F.tanh(self.cnn(x))\n",
    "        return points.view(batch_size, -1, 2)\n",
    "\n",
    "class UnBoundedGridLocNet(nn.Module):\n",
    "\n",
    "    def __init__(self, grid_height, grid_width, target_control_points):\n",
    "        super(UnBoundedGridLocNet, self).__init__()\n",
    "        self.cnn = CNN(grid_height * grid_width * 2)\n",
    "\n",
    "        bias = target_control_points.view(-1)\n",
    "        self.cnn.fc2.bias.data.copy_(bias)\n",
    "        self.cnn.fc2.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        points = self.cnn(x)\n",
    "        return points.view(batch_size, -1, 2)\n",
    "\n",
    "def get_our_model():\n",
    "    model_K1 = make_model('resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
    "\n",
    "    for param in model_K1.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_K1._classifier.weight.requires_grad = True\n",
    "    model_K1._classifier.bias.requires_grad = True\n",
    "\n",
    "    class Main_Network_ResNet(nn.Module):\n",
    "        def __init__(self, original_model):\n",
    "            super(Main_Network_ResNet, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                # stop at conv4\n",
    "                *list(original_model.children())[:-2],\n",
    "            )\n",
    "\n",
    "            self.regions = nn.Sequential(nn.AdaptiveAvgPool2d(2),\n",
    "                        nn.Conv2d(2048, 512, 1),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(512, 128, 1),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ReLU())\n",
    "            \n",
    "            self.regions_ver = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n",
    "                        nn.Conv2d(2048, 512, 1),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(512, 128, 1),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ReLU())\n",
    "            \n",
    "            self.regions_hor = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n",
    "                        nn.Conv2d(2048, 512, 1),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(512, 128, 1),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ReLU())\n",
    "            \n",
    "            self.combine = nn.Sequential(nn.Linear(128*2+ 128*4, 128), nn.ReLU())\n",
    "            \n",
    "            dim1 = 512\n",
    "            dim2 = 256\n",
    "            self.out_ftrs = 256*4\n",
    "\n",
    "            self.classifier = nn.Sequential(nn.Linear(256 + 76 + 2, 128), nn.ReLU())\n",
    "            self.dense = nn.Sequential(nn.Linear(128 + 76 + 2, 100), nn.ReLU())\n",
    "            self.classifier_final = nn.Sequential(nn.Linear(128 + 76 + 2 + 1, 76))\n",
    "#             self.classifier_final = nn.Sequential(nn.Linear(128 + 2, 76))\n",
    "\n",
    "\n",
    "\n",
    "            self.lstm_size = 128\n",
    "            self.lstm = nn.LSTM(128 + 76 + 2 + 1,\n",
    "                                self.lstm_size,\n",
    "                                batch_first=True)\n",
    "\n",
    "\n",
    "        def forward(self, x1, vertical_x1, horizontal_x1, x2, timelines, intervals, noise_list):\n",
    "            batch_size = x1.size(0)\n",
    "            \n",
    "            x1 = self.features(x1)\n",
    "            x1 = self.regions(x1)\n",
    "            x1 = x1.view(-1, 128*4)\n",
    "            \n",
    "            vertical_x1 = self.features(vertical_x1)\n",
    "            vertical_x1 = self.regions_ver(vertical_x1)\n",
    "            vertical_x1 = vertical_x1.view(-1, 128)\n",
    "        \n",
    "            horizontal_x1 = self.features(horizontal_x1)\n",
    "            horizontal_x1 = self.regions_hor(horizontal_x1)\n",
    "            horizontal_x1 = horizontal_x1.view(-1, 128)\n",
    "            \n",
    "            x1 = torch.cat([x1, vertical_x1, horizontal_x1], dim=-1)\n",
    "            x1 = self.combine(x1)\n",
    "\n",
    "            x1 = torch.cat([x1, timelines, x2, intervals, noise_list], dim = -1)\n",
    "            output, state = self.lstm(x1.unsqueeze(0))\n",
    "            x1 = output[0]\n",
    "#             print (intervals.shape, timelines.shape, noise_list.shape)\n",
    "            x1 = torch.cat([x1, x2, intervals, timelines, noise_list], dim=-1)\n",
    "#             x1 = torch.cat([x1, intervals, timelines], dim=-1)\n",
    "            \n",
    "            x1 = self.classifier_final(x1) + x2\n",
    "            return x1, x1, None\n",
    "\n",
    "    main_model = Main_Network_ResNet(model_K1)\n",
    "\n",
    "\n",
    "    model_K = make_model('resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
    "\n",
    "    for param in model_K.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_K._classifier.weight.requires_grad = True\n",
    "    model_K._classifier.bias.requires_grad = True\n",
    "\n",
    "\n",
    "    class Model_Glaucoma(nn.Module):\n",
    "        def __init__(self, main_net):\n",
    "            super(Model_Glaucoma, self).__init__()\n",
    "            self.main_net = main_net\n",
    "            self.lstm_size = main_net.lstm_size\n",
    "\n",
    "        def forward(self, thickness_imgs, vertical_imgs, horizontal_imgs, previous_VFs, timelines, intervals, total_VFs, noise_list, prev_state):\n",
    "            vf_pred, vf_norm, x2_vf_transform_feat = self.main_net(thickness_imgs[0], vertical_imgs[0], horizontal_imgs[0], previous_VFs[0], timelines, intervals, noise_list)\n",
    "            return vf_pred, vf_norm\n",
    "\n",
    "        def zero_state(self, batch_size):\n",
    "            return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                    torch.zeros(1, batch_size, self.lstm_size))\n",
    "\n",
    "    model_all = Model_Glaucoma(main_model)\n",
    "    return model_all\n",
    "\n",
    "def get_model():    \n",
    "    final_model = get_our_model()\n",
    "    return final_model\n",
    "\n",
    "\n",
    "def get_model_noise(pre_trained = False, model_name = None):\n",
    "    model_K = make_model('resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
    "\n",
    "    for param in model_K.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_K._classifier.weight.requires_grad = True\n",
    "    model_K._classifier.bias.requires_grad = True\n",
    "\n",
    "\n",
    "    class InceptionV4Bottom(nn.Module):\n",
    "                def __init__(self, original_model):\n",
    "                    super(InceptionV4Bottom, self).__init__()\n",
    "                    self.features = nn.Sequential(\n",
    "                        # stop at conv4\n",
    "                        *list(original_model.children())[:-1]\n",
    "                    )\n",
    "                    dim1 = 512\n",
    "                    dim2 = 256\n",
    "                    self.num_ftrs = original_model._classifier.in_features    \n",
    "                    self.classifier = nn.Sequential(nn.Linear(self.num_ftrs, dim2), nn.BatchNorm1d(dim2), nn.ReLU(), # Original model\n",
    "                                                    nn.Linear(dim2, NUM_CLASSES))\n",
    "\n",
    "                def forward(self, x1, angle):\n",
    "                    x1 = self.features(x1)\n",
    "                    x1 = x1.view(-1, self.num_ftrs)\n",
    "                    x1 = self.classifier(x1)\n",
    "                    return x1\n",
    "\n",
    "    model_ft = InceptionV4Bottom(model_K)\n",
    "    if pre_trained:\n",
    "        model_ft.load_state_dict(torch.load(model_name))\n",
    "    return model_ft\n",
    "\n",
    "def traing_one_model_noise(dict_pred, model, optimizer, criterion_main, criterion_AE, dataloaders, dataset_sizes,\n",
    "                     NUM_EPOCHS = 25, training_noise = False, BETA_AE_list = [10, 0.05, 0.001]):\n",
    "    history = {'train_loss' : [], 'val_loss': [], 'train_main': [], 'val_main': [], 'train_ae': [], 'val_ae': [], \n",
    "               'train_acc': [], 'val_acc': [],\n",
    "               'train_auc': [], 'val_auc': [], 'train_final_score': [], 'val_final_score': []}\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 10000\n",
    "    BETA_AE = BETA_AE_list[0]\n",
    "    if training_noise:\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print('Epoch {}/{}'.format(epoch, NUM_EPOCHS - 1))\n",
    "            print('-' * 10)\n",
    "            if epoch == 7:\n",
    "                BETA_AE = BETA_AE_list[1]\n",
    "                unfreeze(model, 5)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.0001\n",
    "\n",
    "            if epoch == 15:\n",
    "                BETA_AE = BETA_AE_list[2]\n",
    "        #         BETA_AE = 1\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.00001  \n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(param_group['lr'])\n",
    "                running_loss = 0.0\n",
    "                running_main = 0\n",
    "                running_ae = 0\n",
    "\n",
    "                data_size = dataset_sizes[phase]\n",
    "                gt_data = np.empty((data_size, NUM_CLASSES), dtype=float)\n",
    "                pr_data = np.empty((data_size, NUM_CLASSES), dtype=float)\n",
    "                prMax_data = np.empty((data_size), dtype=float)\n",
    "                gtMax_data = np.empty((data_size), dtype=float)\n",
    "                index_C = 0\n",
    "                # Iterate over data.\n",
    "                for data_batch_s in tqdm(dataloaders[phase]):\n",
    "                    inputs = data_batch_s['image']\n",
    "                    labels = data_batch_s['label'] \n",
    "                    angles = data_batch_s['angle']\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    angles = angles.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs, angles)\n",
    "                        loss_main = criterion_main(outputs, labels.float())\n",
    "                        loss = loss_main\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_main += loss_main.item() * inputs.size(0)\n",
    "                    running_ae += loss_main.item() * inputs.size(0)\n",
    "                    batch_size = inputs.shape[0]\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_main = running_main / dataset_sizes[phase]\n",
    "                epoch_ae = running_ae / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "                print('{} Loss: {:.4f}|Main: {:.4f}| AE: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_main, epoch_ae))\n",
    "                history[phase + '_loss'].append(epoch_loss)\n",
    "                history[phase + '_main'].append(epoch_main)\n",
    "                history[phase + '_ae'].append(epoch_ae)\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        plt.plot(history['train_loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Total loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.savefig('Aug_loss.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history['train_main'])\n",
    "        plt.plot(history['val_main'])\n",
    "        plt.title('Main loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.savefig('Aug_loss.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history['train_ae'])\n",
    "        plt.plot(history['val_ae'])\n",
    "        plt.title('AE loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.savefig('Aug_loss.png')\n",
    "        plt.show()\n",
    "    \n",
    "    MSE_MAP = 0\n",
    "    MAE_MAP = 0\n",
    "    RMSE = 0\n",
    "    MAE_mean = 0\n",
    "    LIST_MAE = np.zeros((0))\n",
    "    list_pred = np.zeros((0,76))\n",
    "    list_GT = np.zeros((0,76))\n",
    "    list_file_path = []\n",
    "    list_string_path = []\n",
    "    model.eval()\n",
    "    phase = 'val'\n",
    "    train_loader_noise = torch.utils.data.DataLoader(dataloaders['train'].dataset,\n",
    "                                                 batch_size=16, shuffle=False,\n",
    "                                                 num_workers=4, drop_last=False)\n",
    "    for data_batch_s in tqdm(train_loader_noise):\n",
    "        inputs = data_batch_s['image']\n",
    "        labels = data_batch_s['label'] \n",
    "        angles = data_batch_s['angle']\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        angles = angles.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs, angles)\n",
    "\n",
    "        labels = labels*STD + MEAN\n",
    "        outputs = outputs*STD + MEAN\n",
    "\n",
    "        outputs[outputs > 40] = 40\n",
    "        outputs[outputs < -1] = -1\n",
    "\n",
    "        MSE_MAP += torch.sum((labels - outputs)**2, dim=0).detach().cpu().numpy()\n",
    "        MAE_MAP += torch.sum(torch.abs(labels - outputs), dim=0).detach().cpu().numpy()\n",
    "        RMSE += np.sqrt(torch.mean((labels - outputs)**2, dim=1).detach().cpu().numpy()).sum()\n",
    "        MAE_mean += torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy().sum()\n",
    "        LIST_MAE = np.append(LIST_MAE, torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy())\n",
    "        list_pred = np.append(list_pred, outputs.cpu().numpy(), axis= 0)\n",
    "        list_GT = np.append(list_GT, labels.cpu().numpy(), axis=0)\n",
    "\n",
    "        list_file_path.extend(data_batch_s['file'])\n",
    "        list_string_path.extend(data_batch_s['string_path'])\n",
    "    MSE_MAP /= dataset_sizes['train']\n",
    "    MAE_MAP /= dataset_sizes['train']\n",
    "    RMSE /= dataset_sizes['train']\n",
    "    MAE_mean /= dataset_sizes['train']\n",
    "    print('Train result: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(MSE_MAP.mean(), MAE_MAP.mean(), RMSE, MAE_mean))\n",
    "    \n",
    "    PATH = './result/baseline/'\n",
    "    for i in tqdm(range(len(list_GT))):\n",
    "        id_temp = list_file_path[i].split('/')[-1].split('.')[0].split('_')[0]\n",
    "        lr_temp = list_file_path[i].split('/')[-1].split('.')[0].split('_')[1]\n",
    "        dict_pred['file'].append(list_string_path[i])\n",
    "        dict_pred['vf_GT'].append(list_GT[i])\n",
    "        dict_pred['vf_pred'].append(list_pred[i])\n",
    "        \n",
    "    MSE_MAP = 0\n",
    "    MAE_MAP = 0\n",
    "    RMSE = 0\n",
    "    MAE_mean = 0\n",
    "    LIST_MAE = np.zeros((0))\n",
    "    list_pred = np.zeros((0,76))\n",
    "    list_GT = np.zeros((0,76))\n",
    "    list_file_path = []\n",
    "    model.eval()\n",
    "    for data_batch_s in tqdm(dataloaders['val']):\n",
    "        inputs = data_batch_s['image']\n",
    "        labels = data_batch_s['label'] \n",
    "        angles = data_batch_s['angle']\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        angles = angles.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs, angles)\n",
    "\n",
    "        labels = labels*STD + MEAN\n",
    "        outputs = outputs*STD + MEAN\n",
    "\n",
    "        outputs[outputs > 40] = 40\n",
    "        outputs[outputs < -1] = -1\n",
    "\n",
    "        MSE_MAP += torch.sum((labels - outputs)**2, dim=0).detach().cpu().numpy()\n",
    "        MAE_MAP += torch.sum(torch.abs(labels - outputs), dim=0).detach().cpu().numpy()\n",
    "        RMSE += np.sqrt(torch.mean((labels - outputs)**2, dim=1).detach().cpu().numpy()).sum()\n",
    "        MAE_mean += torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy().sum()\n",
    "        LIST_MAE = np.append(LIST_MAE, torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy())\n",
    "        list_pred = np.append(list_pred, outputs.cpu().numpy(), axis= 0)\n",
    "        list_GT = np.append(list_GT, labels.cpu().numpy(), axis=0)\n",
    "\n",
    "        list_file_path.extend(data_batch_s['file'])\n",
    "    MSE_MAP /= dataset_sizes['val']\n",
    "    MAE_MAP /= dataset_sizes['val']\n",
    "    RMSE /= dataset_sizes['val']\n",
    "    MAE_mean /= dataset_sizes['val']\n",
    "    print('End result: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(MSE_MAP.mean(), MAE_MAP.mean(), RMSE, MAE_mean))\n",
    "\n",
    "    return MSE_MAP.mean(), MAE_MAP.mean(), RMSE, MAE_mean, model, MSE_MAP, MAE_MAP, dict_pred\n",
    "\n",
    "def traing_one_model(model, optimizer, criterion_main, criterion_AE, dataloaders, dataset_sizes, \n",
    "                     NUM_EPOCHS = 25, BETA_AE_list = [1, 1, 1]):\n",
    "    history = {'train_loss' : [], 'val_loss': [], 'train_main': [], 'val_main': [], 'train_ae': [], 'val_ae': [], \n",
    "               'train_acc': [], 'val_acc': [],\n",
    "               'train_auc': [], 'val_auc': [], 'train_final_score': [], 'val_final_score': []}\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 10000\n",
    "    BETA_AE = BETA_AE_list[0]\n",
    "    mse_hd = MSE_HD_loss(0.7)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print('Epoch {}/{}'.format(epoch, NUM_EPOCHS - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch == 7:\n",
    "            BETA_AE = BETA_AE_list[1]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 0.0001\n",
    "\n",
    "        if epoch == 15:\n",
    "            BETA_AE = BETA_AE_list[2]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 0.00001  \n",
    "\n",
    "        \n",
    "        state_h, state_c = model.zero_state(1)\n",
    "        \n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print(param_group['lr'])\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_main = 0\n",
    "            running_ae = 0\n",
    "\n",
    "            data_size = dataset_sizes[phase]\n",
    "            gt_data = np.empty((data_size, NUM_CLASSES), dtype=float)\n",
    "            pr_data = np.empty((data_size, NUM_CLASSES), dtype=float)\n",
    "            prMax_data = np.empty((data_size), dtype=float)\n",
    "            gtMax_data = np.empty((data_size), dtype=float)\n",
    "            index_C = 0\n",
    "            for data_batch_s in tqdm(dataloaders[phase]):\n",
    "                previous_VFs = data_batch_s['input']\n",
    "                labels = data_batch_s['label']\n",
    "                imgs_data = data_batch_s['imgs']\n",
    "                vertical_imgs_data = data_batch_s['vertical_imgs']\n",
    "                horizontal_imgs_data = data_batch_s['horizontal_imgs']\n",
    "                noise_list = data_batch_s['noise']\n",
    "                timelines = data_batch_s['timeline']\n",
    "                intervals = data_batch_s['interval']\n",
    "                total_VFs = data_batch_s['total_vfs'][:,0,:]\n",
    "                if phase == 'train' and  previous_VFs.shape[1] < 2:\n",
    "                    continue\n",
    "                if previous_VFs.shape[1] == 0 or labels.shape[1] == 0:\n",
    "                    continue\n",
    "                previous_VFs = previous_VFs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                imgs_data = imgs_data.to(device)[:,:-1]\n",
    "                vertical_imgs_data = vertical_imgs_data.to(device)[:,:-1]\n",
    "                horizontal_imgs_data = horizontal_imgs_data.to(device)[:,:-1]\n",
    "                noise_list = noise_list.to(device)\n",
    "                total_VFs = total_VFs.to(device)\n",
    "                intervals = intervals.to(device).reshape(-1,1)\n",
    "                timelines = timelines.to(device)[0, 1:].unsqueeze(-1)\n",
    "                \n",
    "                batch_size_c = noise_list.shape[1] - 1\n",
    "                noise_list = noise_list.view(batch_size_c + 1)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_size = labels.shape[1]\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    mask = noise_list.reshape(-1,1)[:-1]\n",
    "                    outputs, vf_norm = model(imgs_data, vertical_imgs_data, horizontal_imgs_data, previous_VFs[:,:,1:], timelines, intervals, total_VFs, mask, (state_h, state_c))\n",
    "                    vf_preds = noise_list[1:]\n",
    "                    loss_ae = torch.tensor(0)\n",
    "                    y = torch.ones(labels.shape[1]) * TH_REG_OUTPUT \n",
    "                    y = y.cuda()\n",
    "                    outputs = outputs.view(batch_size_c, 76)\n",
    "                    labels = labels.view(batch_size_c, 76)\n",
    "                    \n",
    "                    loss_main = weighted_mse_loss(outputs, labels.float(), torch.exp(TH_REG_OUTPUT - torch.where(vf_preds > TH_REG_OUTPUT, vf_preds, y)))\n",
    "                    loss_norm_cnn =  weighted_mse_loss(vf_norm, labels.float(), torch.exp(TH_REG_OUTPUT - torch.where(vf_preds > TH_REG_OUTPUT, vf_preds, y)))\n",
    "                    loss_hd = mse_hd(outputs, labels.float())\n",
    "                    loss = 1.*loss_main + 1.7*loss_hd\n",
    "                    state_h = state_h.detach()\n",
    "                    state_c = state_c.detach()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * previous_VFs.size(1)\n",
    "                running_main += loss_main.item() * previous_VFs.size(1)\n",
    "                running_ae += loss_ae.item() * previous_VFs.size(1)\n",
    "                batch_size = previous_VFs.shape[1]\n",
    "\n",
    "                state_msg = ('Loss: {:.4f}'.format(loss_main.item()))\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_main = running_main / dataset_sizes[phase]\n",
    "            epoch_ae = running_ae / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f}|Main: {:.4f}| AE: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_main, epoch_ae))\n",
    "            history[phase + '_loss'].append(epoch_loss)\n",
    "            history[phase + '_main'].append(epoch_main)\n",
    "            history[phase + '_ae'].append(epoch_ae)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Total loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig('Aug_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history['train_main'])\n",
    "    plt.plot(history['val_main'])\n",
    "    plt.title('Main loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig('Aug_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history['train_ae'])\n",
    "    plt.plot(history['val_ae'])\n",
    "    plt.title('AE loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig('Aug_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    MSE_MAP = 0\n",
    "    MAE_MAP = 0\n",
    "    RMSE = 0\n",
    "    MAE_mean = 0\n",
    "    LIST_MAE = np.zeros((0))\n",
    "    list_pred = np.zeros((0,76))\n",
    "    list_GT = np.zeros((0,76))\n",
    "    LIST_INTERVALS = np.zeros((0))\n",
    "    LIST_VISITS = np.zeros((0))\n",
    "    list_file_path = []\n",
    "    m_sf = nn.Softmax(dim=2)\n",
    "    num_of_samples = 0\n",
    "    for data_batch_s in tqdm(dataloaders['val']):\n",
    "        previous_VFs = data_batch_s['input']\n",
    "        labels = data_batch_s['label']\n",
    "        imgs_data = data_batch_s['imgs']\n",
    "        vertical_imgs_data = data_batch_s['vertical_imgs']\n",
    "        horizontal_imgs_data = data_batch_s['horizontal_imgs']\n",
    "        noise_list = data_batch_s['noise']\n",
    "        timelines = data_batch_s['timeline']\n",
    "        intervals = data_batch_s['interval']\n",
    "        total_VFs = data_batch_s['total_vfs'][:,0,:]\n",
    "        if phase == 'train' and  previous_VFs.shape[1] < 2:\n",
    "            continue\n",
    "        if previous_VFs.shape[1] == 0 or labels.shape[1] == 0:\n",
    "            continue\n",
    "        previous_VFs = previous_VFs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        imgs_data = imgs_data.to(device)[:,:-1]\n",
    "        vertical_imgs_data = vertical_imgs_data.to(device)[:,:-1]\n",
    "        horizontal_imgs_data = horizontal_imgs_data.to(device)[:,:-1]\n",
    "        noise_list = noise_list.to(device)\n",
    "        total_VFs = total_VFs.to(device)\n",
    "        intervals = intervals.to(device).reshape(-1,1)\n",
    "        timelines = timelines.to(device)[0, 1:].unsqueeze(-1)\n",
    "        \n",
    "        batch_size_c = noise_list.shape[1] - 1\n",
    "        noise_list = noise_list.view(batch_size_c + 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            mask = noise_list.reshape(-1,1)[:-1]\n",
    "            outputs, vf_norm = model(imgs_data, vertical_imgs_data, horizontal_imgs_data, previous_VFs[:,:,1:], timelines, intervals, total_VFs, mask, (state_h, state_c))\n",
    "\n",
    "        labels = labels*STD + MEAN\n",
    "        outputs = outputs*STD + MEAN\n",
    "        \n",
    "        outputs = outputs.view(-1, 76)\n",
    "        labels = labels.view(-1, 76)\n",
    "        \n",
    "        num_of_samples += outputs.shape[0]\n",
    "\n",
    "        outputs[outputs > 40] = 40\n",
    "        outputs[outputs < -1] = -1\n",
    "\n",
    "        MSE_MAP += torch.sum((labels - outputs)**2, dim=0).detach().cpu().numpy()\n",
    "        MAE_MAP += torch.sum(torch.abs(labels - outputs), dim=0).detach().cpu().numpy()\n",
    "        RMSE += np.sqrt(torch.mean((labels - outputs)**2, dim=1).detach().cpu().numpy()).sum()\n",
    "        MAE_mean += torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy().sum()\n",
    "        LIST_MAE = np.append(LIST_MAE, torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy())\n",
    "        list_pred = np.append(list_pred, outputs.cpu().numpy(), axis= 0)\n",
    "        list_GT = np.append(list_GT, labels.cpu().numpy(), axis=0)\n",
    "        LIST_INTERVALS = np.append(LIST_INTERVALS, intervals.cpu().numpy().reshape(-1), axis=0)\n",
    "        LIST_VISITS = np.append(LIST_VISITS, np.arange(1,intervals.shape[0]+1), axis=0)\n",
    "        list_file_path.extend(data_batch_s['string_path'][1:])\n",
    "    MSE_MAP /= num_of_samples\n",
    "    MAE_MAP /= num_of_samples\n",
    "    RMSE /= num_of_samples\n",
    "    MAE_mean /= num_of_samples\n",
    "    print('Validation End result: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(MSE_MAP.mean(), MAE_MAP.mean(), RMSE, MAE_mean))\n",
    "    data_one_fold = {'file':list_file_path, 'GT': list_GT, 'pred': list_pred, 'interval': LIST_INTERVALS, 'visits': LIST_VISITS} \n",
    "    \n",
    "    MSE_MAP_external = 0\n",
    "    MAE_MAP_external = 0\n",
    "    RMSE_external = 0\n",
    "    MAE_mean_external = 0\n",
    "    LIST_MAE_external = np.zeros((0))\n",
    "    list_pred_external = np.zeros((0,76))\n",
    "    list_GT_external = np.zeros((0,76))\n",
    "    LIST_INTERVALS_external = np.zeros((0))\n",
    "    LIST_VISITS_external = np.zeros((0))\n",
    "    list_file_path_external = []\n",
    "    m_sf = nn.Softmax(dim=2)\n",
    "    num_of_samples_external = 0\n",
    "    for data_batch_s in tqdm(dataloaders['external']):\n",
    "        previous_VFs = data_batch_s['input']\n",
    "        labels = data_batch_s['label']\n",
    "        imgs_data = data_batch_s['imgs']\n",
    "        vertical_imgs_data = data_batch_s['vertical_imgs']\n",
    "        horizontal_imgs_data = data_batch_s['horizontal_imgs']\n",
    "        noise_list = data_batch_s['noise']\n",
    "        timelines = data_batch_s['timeline']\n",
    "        intervals = data_batch_s['interval']\n",
    "        total_VFs = data_batch_s['total_vfs'][:,0,:]\n",
    "        if phase == 'train' and  previous_VFs.shape[1] < 2:\n",
    "            continue\n",
    "        if previous_VFs.shape[1] == 0 or labels.shape[1] == 0:\n",
    "            continue\n",
    "        previous_VFs = previous_VFs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        imgs_data = imgs_data.to(device)[:,:-1]\n",
    "        vertical_imgs_data = vertical_imgs_data.to(device)[:,:-1]\n",
    "        horizontal_imgs_data = horizontal_imgs_data.to(device)[:,:-1]\n",
    "        noise_list = noise_list.to(device)\n",
    "        total_VFs = total_VFs.to(device)\n",
    "        intervals = intervals.to(device).reshape(-1,1)\n",
    "        timelines = timelines.to(device)[0, 1:].unsqueeze(-1)\n",
    "        \n",
    "        batch_size_c = noise_list.shape[1] - 1\n",
    "        noise_list = noise_list.view(batch_size_c + 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            mask = noise_list.reshape(-1,1)[:-1]\n",
    "            outputs, vf_norm = model(imgs_data, vertical_imgs_data, horizontal_imgs_data, previous_VFs[:,:,1:], timelines, intervals, total_VFs, mask, (state_h, state_c))\n",
    "\n",
    "\n",
    "        labels = labels*STD + MEAN\n",
    "        outputs = outputs*STD + MEAN\n",
    "        \n",
    "        outputs = outputs.view(-1, 76)\n",
    "        labels = labels.view(-1, 76)\n",
    "        \n",
    "        num_of_samples_external += outputs.shape[0]\n",
    "\n",
    "        outputs[outputs > 40] = 40\n",
    "        outputs[outputs < -1] = -1\n",
    "\n",
    "        MSE_MAP_external += torch.sum((labels - outputs)**2, dim=0).detach().cpu().numpy()\n",
    "        MAE_MAP_external += torch.sum(torch.abs(labels - outputs), dim=0).detach().cpu().numpy()\n",
    "        RMSE_external += np.sqrt(torch.mean((labels - outputs)**2, dim=1).detach().cpu().numpy()).sum()\n",
    "        MAE_mean_external += torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy().sum()\n",
    "        LIST_MAE_external = np.append(LIST_MAE_external, torch.mean(torch.abs(labels - outputs), dim=1).detach().cpu().numpy())\n",
    "        list_pred_external = np.append(list_pred_external, outputs.cpu().numpy(), axis= 0)\n",
    "        list_GT_external = np.append(list_GT_external, labels.cpu().numpy(), axis=0)\n",
    "        LIST_INTERVALS_external = np.append(LIST_INTERVALS_external, intervals.cpu().numpy().reshape(-1), axis=0)\n",
    "        LIST_VISITS_external = np.append(LIST_VISITS_external, np.arange(1,intervals.shape[0]+1), axis=0)\n",
    "        list_file_path_external.extend(data_batch_s['string_path'][1:])\n",
    "    MSE_MAP_external /= num_of_samples_external\n",
    "    MAE_MAP_external /= num_of_samples_external\n",
    "    RMSE_external /= num_of_samples_external\n",
    "    MAE_mean_external /= num_of_samples_external\n",
    "    print('External End result: {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(MSE_MAP_external.mean(), MAE_MAP_external.mean(), RMSE_external, MAE_mean_external))\n",
    "\n",
    "    data_one_fold_external = {'file':list_file_path_external, 'GT': list_GT_external, 'pred': list_pred_external, 'interval': LIST_INTERVALS_external, 'visits': LIST_VISITS_external}\n",
    "    return MSE_MAP.mean(), MAE_MAP.mean(), RMSE, MAE_mean, MSE_MAP_external.mean(), MAE_MAP_external.mean(), RMSE_external, MAE_mean_external, model, MSE_MAP, MAE_MAP, data_one_fold, data_one_fold_external\n",
    "\n",
    "\n",
    "def update_noise(df_all, dict_pred, test_loader_train_noise, normalize = True):\n",
    "    df_all['noise'] = -1.\n",
    "    dict_vfs = {}\n",
    "    dict_ae_GT = {}\n",
    "    dict_ae_pred = {}\n",
    "    dict_encoding_pred = {}\n",
    "    for data_batch_s in tqdm(test_loader_train_noise):\n",
    "        inputs = data_batch_s['input']\n",
    "        labels = data_batch_s['label']\n",
    "        total_vfs = data_batch_s['total_vfs']\n",
    "        if inputs.shape[1] == 0 or labels.shape[1] == 0:\n",
    "            continue\n",
    "        list_str_path_valid = []\n",
    "        vf_preds = []\n",
    "        for t in data_batch_s['string_path']:\n",
    "            if t[0] != 'None':\n",
    "                vf_preds.append(dict_pred['vf_pred'][dict_pred['file'].index(t[0])])\n",
    "                list_str_path_valid.append(t)\n",
    "        vf_preds = np.array(vf_preds)\n",
    "\n",
    "        vf_GT = []\n",
    "        for t in data_batch_s['string_path']:\n",
    "            if t[0] != 'None':\n",
    "                vf_GT.append(dict_pred['vf_GT'][dict_pred['file'].index(t[0])])\n",
    "        vf_GT = np.array(vf_GT)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            diff_vfs = np.mean(np.abs(vf_preds-vf_GT), axis=-1)\n",
    "            if normalize:\n",
    "                diff_vfs = diff_vfs - diff_vfs.min()\n",
    "                if diff_vfs.mean() < 0.001:\n",
    "                    label_noise = np.ones_like(diff_vfs)\n",
    "                else:\n",
    "                    diff_vfs = diff_vfs/diff_vfs.mean()\n",
    "                    label_noise = diff_vfs\n",
    "            else:\n",
    "                label_noise = diff_vfs\n",
    "\n",
    "        if len(list_str_path_valid) == 0:\n",
    "            continue\n",
    "        for i in range(label_noise.shape[0]):\n",
    "            df_all.loc[df_all['string_path'] == list_str_path_valid[i][0], 'noise'] = label_noise[i]\n",
    "\n",
    "    return df_all\n",
    "\n",
    "def training_cross_val(df_all, n_KFold = 5, NUM_EPOCHS = 25, training_noise = False):\n",
    "    lista = []\n",
    "    listb = []\n",
    "    listc = []\n",
    "    listd = []\n",
    "    lista_external = []\n",
    "    listb_external = []\n",
    "    listc_external = []\n",
    "    listd_external = []\n",
    "    i = 0\n",
    "    kf = KFold(n_splits=n_KFold, random_state=14, shuffle=True)\n",
    "    ids_all = df_all['patient_id'].unique()\n",
    "    fold_c = 0\n",
    "    results_all = {}\n",
    "    results_all_external = {}\n",
    "    for train_index, test_index in kf.split(ids_all):\n",
    "        df_train = df_all.loc[df_all['patient_id'].isin(ids_all[train_index])]\n",
    "        df_test = df_all.loc[df_all['patient_id'].isin(ids_all[test_index])]\n",
    "\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "        print (len(df_train), len(df_test))\n",
    "        \n",
    "        dict_pred = {'file': [], 'vf_GT': [], 'vf_pred': []}\n",
    "        dataloaders_noise, dataset_sizes_noise, _ = get_dataloader_noise(df_train, df_test, df_all, df_angle)\n",
    "        \n",
    "        print ('EXP #{}'.format(i))\n",
    "        if training_noise is False:\n",
    "            model = get_model_noise(pre_trained = True, model_name = MODEL_NAME + str(fold_c) + '_ver2.pt')\n",
    "        else:\n",
    "            model = get_model_noise()        \n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        model = model.to(device)\n",
    "        criterion_AE = nn.MSELoss()\n",
    "        criterion_main = nn.MSELoss()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "        if training_noise:\n",
    "            a,b,c,d, model, MSE_MAP, MAE_MAP, dict_pred = traing_one_model_noise(dict_pred, model, optimizer, criterion_main, criterion_AE, dataloaders_noise, dataset_sizes_noise,\n",
    "                                                                NUM_EPOCHS = 5, training_noise = training_noise)\n",
    "            torch.save(model.state_dict(), MODEL_NAME + str(fold_c) + '_ver3.pt')\n",
    "        else:\n",
    "            a,b,c,d, model, MSE_MAP, MAE_MAP, dict_pred = traing_one_model_noise(dict_pred, model, optimizer, criterion_main, criterion_AE, dataloaders_noise, dataset_sizes_noise,\n",
    "                                                                NUM_EPOCHS = 25, training_noise = training_noise)\n",
    "        \n",
    "        #===================================================\n",
    "        \n",
    "        dataloaders, dataset_sizes, _ = get_ver2_dataloader(df_train, df_test, df_all)\n",
    "        # Since for testing we cannot know the GT, set normalize = False\n",
    "        df_all = update_noise(df_all, dict_pred, dataloaders['train'], normalize = False)\n",
    "        dataloaders, dataset_sizes, _ = get_ver2_dataloader(df_train, df_test, df_all)\n",
    "\n",
    "        print ('EXP #{}'.format(i))\n",
    "        model = get_model()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion_AE = nn.MSELoss()\n",
    "        criterion_main = nn.MSELoss()   \n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "        a,b,c,d, a_external,b_external,c_external,d_external, model, MSE_MAP, MAE_MAP, data_one_fold, data_one_fold_external = traing_one_model(model, optimizer, criterion_main, criterion_AE,\n",
    "                                                             dataloaders, dataset_sizes, NUM_EPOCHS = NUM_EPOCHS)\n",
    "        \n",
    "        results_all[fold_c] = data_one_fold\n",
    "        results_all_external[fold_c] = data_one_fold_external\n",
    "        fold_c += 1\n",
    "        lista.append(a)\n",
    "        listb.append(b)\n",
    "        listc.append(c)\n",
    "        listd.append(d)\n",
    "        \n",
    "        lista_external.append(a_external)\n",
    "        listb_external.append(b_external)\n",
    "        listc_external.append(c_external)\n",
    "        listd_external.append(d_external)\n",
    "        i += 1\n",
    "        \n",
    "    save_obj(results_all, 'MICCAIworkshop_model_refactor.pkl')\n",
    "    save_obj(results_all_external, 'MICCAIworkshop_model_refactor_external.pkl')\n",
    "    show_VF(MSE_MAP, 9, 7)\n",
    "    show_VF(MAE_MAP, 11, 7)\n",
    "    print ('='*15)\n",
    "    print('OVERALL: MSE: {:.4f}, MAE: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}'.format(np.array(lista).mean(), np.array(listb).mean(), np.array(listc).mean(), np.array(listd).mean()))\n",
    "    print('External OVERALL: MSE: {:.4f}, MAE: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}'.format(np.array(lista_external).mean(), np.array(listb_external).mean(), np.array(listc_external).mean(), np.array(listd_external).mean()))\n",
    "    return model, dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, dataset_sizes, test_loader_all = get_ver2_dataloader(df_train, df_test, df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH_REG_OUTPUT = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, dataloaders = training_cross_val(df_all, 5, NUM_EPOCHS = 15, training_noise = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
